{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T23:22:44.766025Z",
     "start_time": "2024-09-16T23:22:13.861747Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "# Import relevant metrics from scikit-learn\n",
    "from sklearn.metrics import calinski_harabasz_score, silhouette_score, davies_bouldin_score\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "raw_data = pd.read_csv(\"dataset_exercise_5_clustering_highway_traffic.csv\",sep=';')\n",
    "\n",
    "# Sort the DataFrame 'data_df' by columns \"Date\" and \"Interval_5\"\n",
    "raw_data.sort_values([\"Date\", \"Interval_5\"])\n",
    "\n",
    "# Extract unique dates from the sorted DataFrame\n",
    "days = np.unique(raw_data[['Date']].values.ravel())\n",
    "# Calculate the total number of unique days\n",
    "ndays = len(days)\n",
    "\n",
    "# Group the DataFrame 'data_df' by the \"Date\" column\n",
    "day_subsets_df = raw_data.groupby([\"Date\"])\n",
    "\n",
    "# Define the total number of 5-minute intervals in a day\n",
    "nintvals = 288\n",
    "\n",
    "# Create a matrix 'vectorized_day_dataset' filled with NaN values\n",
    "vectorized_day_dataset = np.zeros((ndays, nintvals))\n",
    "vectorized_day_dataset.fill(np.nan)\n",
    "\n",
    "# Loop through each unique day\n",
    "for i in range(0, ndays):\n",
    "    # Get the DataFrame corresponding to the current day\n",
    "    df_t = day_subsets_df.get_group(days[i])\n",
    "\n",
    "    # Loop through each row in the current day's DataFrame\n",
    "    for j in range(len(df_t)):\n",
    "        # Get the current day's DataFrame\n",
    "        df_t = day_subsets_df.get_group(days[i])\n",
    "\n",
    "        # Extract the \"Interval_5\" and \"flow\" values and populate 'vectorized_day_dataset'\n",
    "        vectorized_day_dataset[i, df_t.iloc[j][\"Interval_5\"]] = df_t.iloc[j][\"flow\"]\n",
    "\n",
    "# Print the resulting 'vectorized_day_dataset'\n",
    "#print(vectorized_day_dataset)\n",
    "\n",
    "nans_per_day = np.sum(np.isnan(vectorized_day_dataset),1)\n",
    "\n",
    "n_clusters = 10\n",
    "clusters = None\n",
    "vectorized_day_dataset_no_nans = vectorized_day_dataset[np.where(nans_per_day == 0)[0],:]\n",
    "days_not_nans = days[np.where(nans_per_day == 0)[0]]\n",
    "X = vectorized_day_dataset_no_nans\n",
    "\n",
    "# Lista de métodos de clustering a aplicar\n",
    "methods = {\n",
    "    'KMeans': KMeans(n_clusters=n_clusters, random_state=0, n_init=\"auto\"),\n",
    "    'Agglomerative': AgglomerativeClustering(n_clusters=n_clusters,metric='euclidean', linkage='ward'),\n",
    "    'DBSCAN': DBSCAN(eps=500, min_samples = 2),\n",
    "    'GMM': GaussianMixture(n_components=n_clusters)\n",
    "}\n",
    "\n",
    "# Inicialización de diccionarios para almacenar los resultados\n",
    "results = {\n",
    "    'method': [],\n",
    "    'silhouette': [],\n",
    "    'davies_bouldin': [],\n",
    "    'calinski_harabasz': []\n",
    "}\n",
    "\n",
    "# Evaluación de cada método\n",
    "for name, model in methods.items():\n",
    "    if name == 'GMM':\n",
    "        labels = model.fit_predict(X)\n",
    "    else:\n",
    "        model.fit(X)\n",
    "        labels = model.labels_\n",
    "\n",
    "    # Calcular métricas de evaluación\n",
    "    silhouette_avg = silhouette_score(X, labels)\n",
    "    davies_bouldin_avg = davies_bouldin_score(X, labels)\n",
    "    calinski_harabasz_avg = calinski_harabasz_score(X, labels)\n",
    "    \n",
    "    # Almacenar resultados\n",
    "    results['method'].append(name)\n",
    "    results['silhouette'].append(silhouette_avg)\n",
    "    results['davies_bouldin'].append(davies_bouldin_avg)\n",
    "    results['calinski_harabasz'].append(calinski_harabasz_avg)\n",
    "\n",
    "# Mostrar resultados\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n",
    "\n",
    "# Calculate the Silhouette Score\n",
    "SC_score = silhouette_score(vectorized_day_dataset_no_nans, labels)\n",
    "# Silhouette Score measures the quality of clusters, higher values indicate better separation.\n",
    "\n",
    "# Calculate the Davies-Bouldin Score\n",
    "DB_score = davies_bouldin_score(vectorized_day_dataset_no_nans, labels)\n",
    "# Davies-Bouldin Score measures the average similarity between each cluster and its most similar cluster, lower values indicate better separation.\n",
    "\n",
    "# Calculate the Calinski-Harabasz Score\n",
    "CH_score = calinski_harabasz_score(vectorized_day_dataset_no_nans, labels)\n",
    "# Calinski-Harabasz Score measures the ratio of between-cluster variance to within-cluster variance, higher values indicate better separation.\n",
    "\n",
    "# Print the computed cluster quality scores\n",
    "print('Silhouette Score:', SC_score)\n",
    "print('Davies-Bouldin Score:', DB_score)\n",
    "print('Calinski-Harabasz Score:', CH_score)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jecla\\AppData\\Local\\Temp\\ipykernel_20380\\2275508100.py:31: FutureWarning: When grouping with a length-1 list-like, you will need to pass a length-1 tuple to get_group in a future version of pandas. Pass `(name,)` instead of `name` to silence this warning.\n",
      "  df_t = day_subsets_df.get_group(days[i])\n",
      "C:\\Users\\jecla\\AppData\\Local\\Temp\\ipykernel_20380\\2275508100.py:36: FutureWarning: When grouping with a length-1 list-like, you will need to pass a length-1 tuple to get_group in a future version of pandas. Pass `(name,)` instead of `name` to silence this warning.\n",
      "  df_t = day_subsets_df.get_group(days[i])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          method  silhouette  davies_bouldin  calinski_harabasz\n",
      "0         KMeans    0.188830        1.743424          81.054500\n",
      "1  Agglomerative    0.215062        1.415068          78.955196\n",
      "2         DBSCAN   -0.027721        2.379572          35.327906\n",
      "3            GMM    0.179386        1.743296          81.716902\n",
      "Silhouette Score: 0.17938600077013028\n",
      "Davies-Bouldin Score: 1.7432964360126288\n",
      "Calinski-Harabasz Score: 81.71690182137304\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Internal evaluation"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## External evaluation with short-term prediction"
   ],
   "metadata": {
    "id": "EtggY3IXZVYO"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "First, lets load the evaluation dataset used of evaluating short-term prediction accuracy, vectorize it to day vectors and remove missing values."
   ],
   "metadata": {
    "id": "a8GM5qTBZZgt"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Read the evaluation dataset from a CSV file\n",
    "data_eval_df = pd.read_csv(\"evaluation_dataset_exercise_5_clustering_highway_traffic.csv\", sep=\";\")\n",
    "\n",
    "# Sort the evaluation DataFrame by columns \"Date\" and \"Interval_5\"\n",
    "data_eval_df.sort_values([\"Date\", \"Interval_5\"])\n",
    "\n",
    "# Extract unique dates from the sorted evaluation DataFrame\n",
    "days_eval = np.unique(data_eval_df[['Date']].values.ravel())\n",
    "# Calculate the total number of unique days in the evaluation dataset\n",
    "ndays_eval = len(days_eval)\n",
    "\n",
    "# Group the evaluation DataFrame by the \"Date\" column\n",
    "day_eval_subsets_df = data_eval_df.groupby([\"Date\"])\n",
    "\n",
    "# Initialize a matrix 'vectorized_day_dataset_eval' filled with NaN values\n",
    "vectorized_day_dataset_eval = np.zeros((ndays_eval, nintvals))\n",
    "vectorized_day_dataset_eval.fill(np.nan)\n",
    "# This section initializes a 2D array to store the evaluation dataset and fills it with NaN values.\n",
    "\n",
    "# Loop through each unique day in the evaluation dataset\n",
    "for i in range(0, ndays_eval):\n",
    "    # Get the DataFrame corresponding to the current day\n",
    "    df_t = day_eval_subsets_df.get_group(days_eval[i])\n",
    "\n",
    "    # Loop through each row in the current day's DataFrame\n",
    "    for j in range(len(df_t)):\n",
    "        # Get the current day's DataFrame (this line is redundant)\n",
    "        df_t = day_eval_subsets_df.get_group(days_eval[i])\n",
    "\n",
    "        # Extract the \"Interval_5\" and \"flow\" values and populate 'vectorized_day_dataset_eval'\n",
    "        vectorized_day_dataset_eval[i, df_t.iloc[j][\"Interval_5\"]] = df_t.iloc[j][\"flow\"]\n",
    "\n",
    "# Print the resulting 'vectorized_day_dataset_eval'\n",
    "print(vectorized_day_dataset_eval)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-1dqEbTgZfnx",
    "outputId": "2d15eba3-caf9-4f8d-878b-35d030c9e099",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1694445209155,
     "user_tz": -120,
     "elapsed": 22375,
     "user": {
      "displayName": "Matej Cebecauer",
      "userId": "09411166241023626672"
     }
    },
    "ExecuteTime": {
     "end_time": "2024-09-16T23:23:04.337580Z",
     "start_time": "2024-09-16T23:22:56.096630Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jecla\\AppData\\Local\\Temp\\ipykernel_20380\\2178683536.py:23: FutureWarning: When grouping with a length-1 list-like, you will need to pass a length-1 tuple to get_group in a future version of pandas. Pass `(name,)` instead of `name` to silence this warning.\n",
      "  df_t = day_eval_subsets_df.get_group(days_eval[i])\n",
      "C:\\Users\\jecla\\AppData\\Local\\Temp\\ipykernel_20380\\2178683536.py:28: FutureWarning: When grouping with a length-1 list-like, you will need to pass a length-1 tuple to get_group in a future version of pandas. Pass `(name,)` instead of `name` to silence this warning.\n",
      "  df_t = day_eval_subsets_df.get_group(days_eval[i])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[35. 29. 32. ... 62. 66. 71.]\n",
      " [44. 44. 51. ... 30. 31. 23.]\n",
      " [21. 22. 17. ... 20. 22. 22.]\n",
      " ...\n",
      " [17. 17. 20. ... 35. 25. 27.]\n",
      " [37. 25. 30. ... 37. 36. 49.]\n",
      " [42. 28. 32. ... 45. 49. 38.]]\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "source": [
    "# Calculate the total number of NaN values in the evaluation dataset\n",
    "print('Number of NaNs:', np.sum(np.isnan(vectorized_day_dataset_eval)))\n",
    "\n",
    "# Calculate the rate of NaN values in the evaluation dataset\n",
    "print('Rate of NaNs:', np.sum(np.isnan(vectorized_day_dataset_eval)) / (ndays_eval * nintvals))\n",
    "\n",
    "# Calculate the number of days with missing values\n",
    "nans_per_day_eval = np.sum(np.isnan(vectorized_day_dataset_eval), 1)\n",
    "print('Number of days with missing values:', np.size(np.where(nans_per_day_eval > 0)))\n",
    "\n",
    "# Filter out days with no missing values and create a new dataset\n",
    "vectorized_day_dataset_no_nans_eval = vectorized_day_dataset_eval[np.where(nans_per_day_eval == 0)[0], :]\n",
    "days_not_nans_eval = days_eval[np.where(nans_per_day_eval == 0)[0]]\n",
    "\n",
    "# Calculate the final number of days in the evaluation dataset after removing missing values\n",
    "print('Final number of days in evaluation dataset:', len(days_not_nans_eval))\n",
    "\n",
    "# Print the list of days in the evaluation dataset with no missing values\n",
    "print('List of days without missing values:', days_not_nans_eval)\n",
    "\n",
    "# Calculate the total number of days in the filtered evaluation dataset\n",
    "ndays_eval_not_nans = len(days_not_nans_eval)"
   ],
   "metadata": {
    "id": "nNUDJ57Uj7JZ",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1694445220724,
     "user_tz": -120,
     "elapsed": 247,
     "user": {
      "displayName": "Matej Cebecauer",
      "userId": "09411166241023626672"
     }
    },
    "outputId": "483484cb-5c40-4d8d-84c8-bab6283d2930",
    "ExecuteTime": {
     "end_time": "2024-09-16T23:23:28.081899Z",
     "start_time": "2024-09-16T23:23:28.062877Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaNs: 96\n",
      "Rate of NaNs: 0.004166666666666667\n",
      "Number of days with missing values: 11\n",
      "Final number of days in evaluation dataset: 69\n",
      "List of days without missing values: [20220108 20220109 20220131 20220204 20220209 20220210 20220211 20220223\n",
      " 20220226 20220227 20220302 20220304 20220305 20220306 20220310 20220314\n",
      " 20220315 20220321 20220323 20220326 20220403 20220406 20220416 20220418\n",
      " 20220421 20220422 20220425 20220427 20220428 20220503 20220505 20220514\n",
      " 20220519 20220521 20220522 20220526 20220530 20220601 20220603 20220609\n",
      " 20220616 20220619 20220623 20220628 20220704 20220711 20220712 20220904\n",
      " 20220910 20220911 20220920 20220921 20220925 20220927 20220929 20220930\n",
      " 20221005 20221022 20221024 20221114 20221116 20221121 20221122 20221213\n",
      " 20221216 20221218 20221220 20221223 20221230]\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now when the dataset for evaluation is ready, below is a script for evaluating short-term prediction performance.\n",
    "\n",
    "Prediction works like this for each day at current interval *j*; we use 5 last past intervals to find the closest centroid. This average centroid is used as the source of the prediction for future time interval *j+1*. This directly measures how the representative pattern matches the new future days not part of your training and thus evaluates the clustering to external data and not as internal metrics."
   ],
   "metadata": {
    "id": "-_Yc2vg9gUrJ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Import the pairwise_distances function from scikit-learn's metrics library\n",
    "import sklearn.metrics.pairwise as dis_lib\n",
    "\n",
    "# Define a function to find the closest centroid to a new data point within a specified day-time interval range\n",
    "def find_the_closest_centroid(centroids, new_day, from_interval: int, to_interval: int):\n",
    "    closest_centroid = None\n",
    "    closest_dist = None\n",
    "\n",
    "    # Iterate through each centroid\n",
    "    for i in range(0, len(centroids)):\n",
    "        # Calculate the Euclidean distance between the centroid and the new data point\n",
    "        ed_t = dis_lib.paired_distances(centroids[i], new_day, metric='euclidean')\n",
    "\n",
    "        # Check if the current centroid is closer than the previously closest one\n",
    "        if closest_centroid is None or closest_dist > ed_t:\n",
    "            closest_centroid = i\n",
    "            closest_dist = ed_t\n",
    "\n",
    "    return closest_centroid\n"
   ],
   "metadata": {
    "id": "6259sc-sgVE8",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1694445253487,
     "user_tz": -120,
     "elapsed": 26262,
     "user": {
      "displayName": "Matej Cebecauer",
      "userId": "09411166241023626672"
     }
    },
    "outputId": "ad98f258-0ec0-432d-b839-0561fb9009b3",
    "ExecuteTime": {
     "end_time": "2024-09-16T23:23:38.431196Z",
     "start_time": "2024-09-16T23:23:38.411703Z"
    }
   },
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T23:23:41.689407Z",
     "start_time": "2024-09-16T23:23:39.936710Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from matplotlib.patches import Polygon\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib import gridspec\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib import colors\n",
    "\n",
    "def assign_colors(n_clusters, days, assigments):\n",
    "\n",
    "    days_colors = []\n",
    "    color_to_cluster = []\n",
    "    style_to_cluster = []\n",
    "    weekend_colors = ['#67001f','#d6604d','#fdae61','#f46d43','#d53e4f','#9e0142','#f768a1','#f1c232']#,'#fe9929','#cc4c02','#e31a1c','#737373','#bdbdbd','#252525','#bcbddc']\n",
    "#    weekend_school_colors = ['#c2a5cf','#f1b6da','#8e0152','#c51b7d','#de77ae','#ae017e','#fcc5c0','#e31a1c','#737373','#bdbdbd']\n",
    "#    bank_holidays_colors = ['#543005','#dfc27d','#bf812d','#8c510a']\n",
    "    mixed_colors = ['#4d4d4d','#35978f','#bababa','#878787']\n",
    "    weekday_colors = ['#a6cee3','#1f78b4','#b2df8a','#33a02c','#cab2d6','#6a3d9a','#b15928','#8dd3c7','#bebada','#fb8072','#b3de69','#bc80bd','#fccde5','#ccebc5','#35978f','#80cdc1']\n",
    "\n",
    "    cluster_id_weekdays_share = []\n",
    "    cluster_id_weekend_share = []\n",
    "    cluster_id_all_days = []\n",
    "\n",
    "    for i in range(0,n_clusters):\n",
    "        color_to_cluster.append(None)\n",
    "        style_to_cluster.append(None)\n",
    "        cluster_id_weekdays_share.append(0)\n",
    "        cluster_id_weekend_share.append(0)\n",
    "        cluster_id_all_days.append(0)\n",
    "\n",
    "    for i in range(0,len(days)):\n",
    "        #print(i,assigments[i],len(assigments),len(cluster_id_all_days))\n",
    "        if assigments[i] is not None:\n",
    "            cluster_id_all_days[assigments[i]] += 1\n",
    "            if '-' in str(days[i]):\n",
    "                pomT = datetime.datetime.strptime(str(days[i]),'%Y-%m-%d')\n",
    "            else:\n",
    "                pomT = datetime.datetime.strptime(str(days[i]),'%Y%m%d')\n",
    "\n",
    "            if int(pomT.weekday()) < 5:\n",
    "                cluster_id_weekdays_share[assigments[i]] += 1\n",
    "            else:\n",
    "                cluster_id_weekend_share[assigments[i]] += 1\n",
    "\n",
    "    print('cluster_id_weekdays_share',cluster_id_weekdays_share)\n",
    "    print('cluster_id_weekend_share',cluster_id_weekend_share)\n",
    "    for i in range(0,len(days)):\n",
    "        if assigments[i] is not None:\n",
    "            cluster_idx = assigments[i]\n",
    "            if '-' in str(days[i]):\n",
    "                pomT = datetime.datetime.strptime(str(days[i]),'%Y-%m-%d')\n",
    "            else:\n",
    "                pomT = datetime.datetime.strptime(str(days[i]),'%Y%m%d')\n",
    "            if color_to_cluster[assigments[i]] is None:\n",
    "                if cluster_id_weekend_share[cluster_idx] / float(cluster_id_all_days[cluster_idx]) > 0.6:\n",
    "                        color_to_cluster[assigments[i]] = weekend_colors.pop()\n",
    "                        style_to_cluster[assigments[i]] = ':'\n",
    "                elif cluster_id_weekdays_share[cluster_idx] / float(cluster_id_all_days[cluster_idx]) > 0.6:\n",
    "                        color_to_cluster[assigments[i]] = weekday_colors.pop(0)\n",
    "                        style_to_cluster[assigments[i]] = '-'\n",
    "                else:\n",
    "                    color_to_cluster[assigments[i]] = mixed_colors.pop()\n",
    "                    style_to_cluster[assigments[i]] = ':'\n",
    "\n",
    "            days_colors.append(color_to_cluster[assigments[i]])\n",
    "        else:\n",
    "            days_colors.append(None)\n",
    "\n",
    "    return days_colors,color_to_cluster,style_to_cluster\n",
    "\n",
    "\n",
    "def calmap(ax, year, data, days, assigments, n_clusters,days_colors,color_to_cluster,\n",
    "           limit_graphics=False):\n",
    "\n",
    "    ax.tick_params('x', length=0, labelsize=\"medium\", which='major')\n",
    "    ax.tick_params('y', length=0, labelsize=\"x-small\", which='major')\n",
    "\n",
    "    # Month borders\n",
    "\n",
    "    xticks, labels = [], []\n",
    "    start = datetime.datetime(year,1,1).weekday()\n",
    "\n",
    "    for month in range(1,13):\n",
    "\n",
    "        first = datetime.datetime(year, month, 1)\n",
    "        last = first + relativedelta(months=1, days=-1)\n",
    "\n",
    "        y0 = first.weekday()\n",
    "        y1 = last.weekday()\n",
    "        x0 = (int(first.strftime(\"%j\"))+start-1)//7\n",
    "        x1 = (int(last.strftime(\"%j\"))+start-1)//7\n",
    "\n",
    "        P = [ (x0,   y0), (x0,    7),  (x1,   7),\n",
    "              (x1,   y1+1), (x1+1,  y1+1), (x1+1, 0),\n",
    "              (x0+1,  0), (x0+1,  y0) ]\n",
    "\n",
    "        xticks.append(x0 +(x1-x0+1)/2)\n",
    "        labels.append(first.strftime(\"%b\"))\n",
    "        poly = Polygon(P, edgecolor=\"black\", facecolor=\"None\",\n",
    "\n",
    "                       linewidth=1, zorder=20, clip_on=False)\n",
    "\n",
    "        ax.add_artist(poly)\n",
    "\n",
    "    line = Line2D([0,53],[5,5],linewidth=1, zorder = 20,color=\"black\",linestyle='dashed')\n",
    "    ax.add_artist(line)\n",
    "\n",
    "    if not limit_graphics:\n",
    "        ax.set_xticks(xticks)\n",
    "        ax.set_xticklabels(labels)\n",
    "        ax.set_yticks(0.5 + np.arange(7))\n",
    "        ax.set_yticklabels([\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"])\n",
    "        ax.set_title(\"{}\".format(year), weight=\"semibold\")\n",
    "    else:\n",
    "        plt.tick_params(\n",
    "            axis='x',          # changes apply to the x-axis\n",
    "            which='both',      # both major and minor ticks are affected\n",
    "            bottom=False,      # ticks along the bottom edge are off\n",
    "            top=False,         # ticks along the top edge are off\n",
    "            labelbottom=False)\n",
    "        plt.tick_params(\n",
    "            axis='y',          # changes apply to the x-axis\n",
    "            which='both',      # both major and minor ticks are affected\n",
    "            left=False,      # ticks along the bottom edge are off\n",
    "            right=False,         # ticks along the top edge are off\n",
    "            labelleft=False)\n",
    "\n",
    "    # Clearing first and last day from the data\n",
    "    valid = datetime.datetime(year, 1, 1).weekday()\n",
    "    data[:valid,0] = np.nan\n",
    "    valid = datetime.datetime(year, 12, 31).weekday()\n",
    "    # data[:,x1+1:] = np.nan\n",
    "    data[valid+1:,x1] = np.nan\n",
    "\n",
    "    for i in range(0,len(days)):\n",
    "        if '-' in str(days[i]):\n",
    "            pomT = datetime.datetime.strptime(str(days[i]),'%Y-%m-%d')\n",
    "        else:\n",
    "            pomT = datetime.datetime.strptime(str(days[i]),'%Y%m%d')\n",
    "        week_number = int(pomT.strftime(\"%W\"))\n",
    "        day_of_week = int(pomT.weekday())\n",
    "        data[day_of_week,week_number] = assigments[i]\n",
    "\n",
    "\n",
    "    act_date = datetime.datetime(year,1,1)\n",
    "    while (act_date.year == year):\n",
    "\n",
    "        week_number = int(act_date.strftime(\"%W\"))\n",
    "        day_of_week = int(act_date.weekday())\n",
    "        doy_id = act_date.timetuple().tm_yday\n",
    "        if doy_id<5 and week_number > 53:\n",
    "            week_number = 0\n",
    "\n",
    "        act_date = act_date + datetime.timedelta(days=1)\n",
    "\n",
    "    #pomT = datetime.datetime.strptime('2017-01-01','%Y-%m-%d')\n",
    "    #week_number = int(pomT.strftime(\"%V\"))\n",
    "    #day_of_week = int(pomT.weekday())\n",
    "    #print(week_number,day_of_week)\n",
    "    #doy_id = pomT.timetuple().tm_yday\n",
    "    #if doy_id<5 and week_number > 0:\n",
    "    #    week_number = 0\n",
    "    #data[day_of_week,week_number] = len(clusters)+10\n",
    "\n",
    "    # Showing data\n",
    "    cmap = plt.cm.spring  # Can be any colormap that you want after the cm\n",
    "    cmap.set_bad(color='white')\n",
    "\n",
    "    #ax.imshow(data, extent=[0,53,0,7], zorder=10, vmin=0, vmax=len(clusters)+10,\n",
    "    #          cmap=cmap, origin=\"lower\", alpha=.75)\n",
    "\n",
    "    cmap = colors.ListedColormap(color_to_cluster)\n",
    "    bounds=[-0.1]\n",
    "    step = 1\n",
    "    for i in range(0,n_clusters):\n",
    "        bounds.append(i-0.1+step)\n",
    "    norm = colors.BoundaryNorm(bounds, cmap.N)\n",
    "    #print(color_to_cluster)\n",
    "   #print(bounds)\n",
    "    #print(norm)\n",
    "\n",
    "    #print(color_to_cluster)\n",
    "    #print(bounds)\n",
    "    #print(cmap)\n",
    "    #exit(0)\n",
    "\n",
    "    ax.imshow(data, extent=[0,53,0,7], zorder=10, interpolation='nearest', origin='lower',cmap=cmap, norm=norm)\n",
    "\n",
    "def make_calendar_visualization_figure(days,assigments,n_clusters,years,days_colors,color_to_cluster,\n",
    "                                       save_figure: str = None, show_figure:bool = True, limit_graphics = False):\n",
    "\n",
    "    fig = plt.figure(figsize=(8,1.5*len(years)), dpi=100)\n",
    "    X = np.linspace(-1,1, 53*7)\n",
    "\n",
    "    for i, obj in enumerate(years):\n",
    "\n",
    "        pom_s = str(len(years))+'1'+str(i+1)\n",
    "        print(pom_s)\n",
    "\n",
    "        ax = plt.subplot(int(pom_s), xlim=[0, 53], ylim=[0, 7], frameon=False, aspect=1)\n",
    "        I = 1.2 - np.cos(X.ravel()) + np.random.normal(0,.2, X.size)\n",
    "        I = I.reshape(53,7).T\n",
    "        I.fill(np.nan)\n",
    "        calmap(ax, int(obj), I.reshape(53,7).T, days, assigments, n_clusters,days_colors,color_to_cluster, limit_graphics)\n",
    "\n",
    "    #   ax = plt.subplot(212, xlim=[0,53], ylim=[0,7], frameon=False, aspect=1)\n",
    "    #  I = 1.1 - np.cos(X.ravel()) + np.random.normal(0,.2, X.size)\n",
    "    #   calmap(ax, 2018, I.reshape(53,7).T)\n",
    "\n",
    "    #ax = plt.subplot(313, xlim=[0,53], ylim=[0,7], frameon=False, aspect=1)\n",
    "    #I = 1.0 - np.cos(X.ravel()) + np.random.normal(0,.2, X.size)\n",
    "    #calmap(ax, 2019, I.reshape(53,7).T)\n",
    "    if save_figure:\n",
    "        plt.savefig(save_figure)\n",
    "\n",
    "    if show_figure or save_figure is None:\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def make_figure_centroids(x,y,color_to_cluster,style_to_cluster,cluster_ids,minY = None,maxY = None,\n",
    "                          save_figure: str = None, show_figure:bool = True):\n",
    "\n",
    "    #print(color_to_cluster)\n",
    "    fig = plt.figure(figsize=(8,3))\n",
    "    ax = fig.add_subplot(111)\n",
    "    for i in range(0,len(x)):\n",
    "        #print(i,color_to_cluster[i],style_to_cluster[i])\n",
    "        #print(y[i])\n",
    "        ax.plot(x[i],y[i],style_to_cluster[i], color=color_to_cluster[i], label=str(cluster_ids[i]))\n",
    "    ax.set_xlabel('Time of day')\n",
    "    ax.set_ylabel('Flow')\n",
    "    if minY is not None and maxY is not None:\n",
    "        ax.set_ylim([minY, maxY])\n",
    "    plt.legend()\n",
    "\n",
    "    if save_figure:\n",
    "        plt.savefig(save_figure)\n",
    "\n",
    "    if show_figure or save_figure is None:\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Calculate the number of clusters by finding unique values in 'cluster_labels'\n",
    "n_clusters_t = len(np.unique(labels))\n",
    "\n",
    "# Assign colors to days based on clusters\n",
    "days_colors, color_to_cluster, style_to_cluster = assign_colors(n_clusters_t, days_not_nans, labels)\n",
    "# The function 'assign_colors' is used to determine colors and styles for visualization.\n",
    "\n",
    "# Create a calendar visualization figure\n",
    "make_calendar_visualization_figure(days_not_nans, labels, n_clusters_t, [2021], days_colors,\n",
    "                                   color_to_cluster, save_figure=None)\n",
    "# This function 'make_calendar_visualization_figure' is used to generate a visualization based on the provided data and parameters.\n",
    "# 'days_not_nans' are the days, 'labels' are the cluster labels, 'n_clusters_t' is the number of clusters,\n",
    "# '[2021]' represents the year, 'days_colors' represent the assigned colors for each day, 'color_to_cluster' maps colors to clusters,\n",
    "# and 'save_figure' is an optional parameter to save the generated figure (can be None if not saving)."
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datetime' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[23], line 249\u001B[0m\n\u001B[0;32m    246\u001B[0m n_clusters_t \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(np\u001B[38;5;241m.\u001B[39munique(labels))\n\u001B[0;32m    248\u001B[0m \u001B[38;5;66;03m# Assign colors to days based on clusters\u001B[39;00m\n\u001B[1;32m--> 249\u001B[0m days_colors, color_to_cluster, style_to_cluster \u001B[38;5;241m=\u001B[39m \u001B[43massign_colors\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn_clusters_t\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdays_not_nans\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    250\u001B[0m \u001B[38;5;66;03m# The function 'assign_colors' is used to determine colors and styles for visualization.\u001B[39;00m\n\u001B[0;32m    251\u001B[0m \n\u001B[0;32m    252\u001B[0m \u001B[38;5;66;03m# Create a calendar visualization figure\u001B[39;00m\n\u001B[0;32m    253\u001B[0m make_calendar_visualization_figure(days_not_nans, labels, n_clusters_t, [\u001B[38;5;241m2021\u001B[39m], days_colors,\n\u001B[0;32m    254\u001B[0m                                    color_to_cluster, save_figure\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m)\n",
      "Cell \u001B[1;32mIn[23], line 37\u001B[0m, in \u001B[0;36massign_colors\u001B[1;34m(n_clusters, days, assigments)\u001B[0m\n\u001B[0;32m     35\u001B[0m     pomT \u001B[38;5;241m=\u001B[39m datetime\u001B[38;5;241m.\u001B[39mdatetime\u001B[38;5;241m.\u001B[39mstrptime(\u001B[38;5;28mstr\u001B[39m(days[i]),\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124mY-\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124mm-\u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     36\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 37\u001B[0m     pomT \u001B[38;5;241m=\u001B[39m \u001B[43mdatetime\u001B[49m\u001B[38;5;241m.\u001B[39mdatetime\u001B[38;5;241m.\u001B[39mstrptime(\u001B[38;5;28mstr\u001B[39m(days[i]),\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124mY\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124mm\u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     39\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mint\u001B[39m(pomT\u001B[38;5;241m.\u001B[39mweekday()) \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m5\u001B[39m:\n\u001B[0;32m     40\u001B[0m     cluster_id_weekdays_share[assigments[i]] \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'datetime' is not defined"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T23:24:20.498290Z",
     "start_time": "2024-09-16T23:23:44.455046Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize a list to store centroid data\n",
    "centroids = []\n",
    "\n",
    "# Calculate centroids for each cluster\n",
    "for i in range(0, n_clusters_t):\n",
    "    centroid = np.nanmean(vectorized_day_dataset_no_nans[np.where(labels == i)[0], :], 0).reshape(1, nintvals)\n",
    "    centroids.append(centroid)\n",
    "\n",
    "# Define the number of past intervals to consider for classification\n",
    "n_past_intervals_for_classification = 5\n",
    "\n",
    "# Initialize variables to calculate accuracy metrics\n",
    "total_mae = 0\n",
    "total_mape = 0\n",
    "prediction_counts = 0\n",
    "\n",
    "# Loop through each day in the evaluation dataset with no missing values\n",
    "for i in range(0, ndays_eval_not_nans):\n",
    "    # Loop through intervals from n_past_intervals_for_classification to nintvals - 1\n",
    "    for j in range(n_past_intervals_for_classification, nintvals - 1):\n",
    "        # Find the closest centroid for the current data point\n",
    "        centroid_index = find_the_closest_centroid(centroids, vectorized_day_dataset_no_nans_eval[i].reshape(1, nintvals), j - n_past_intervals_for_classification, j)\n",
    "\n",
    "        # Predict the value for the next interval\n",
    "        predicted_value = centroids[centroid_index][0, j + 1]\n",
    "\n",
    "        # Calculate Mean Absolute Error (MAE) and Mean Absolute Percentage Error (MAPE)\n",
    "        mae_t = abs(predicted_value - vectorized_day_dataset_no_nans_eval[i][j + 1])\n",
    "        mape_t = abs(predicted_value - vectorized_day_dataset_no_nans_eval[i][j + 1]) / float(vectorized_day_dataset_no_nans_eval[i][j + 1])\n",
    "\n",
    "        # Accumulate MAE, MAPE, and count of predictions\n",
    "        total_mae += mae_t\n",
    "        total_mape += mape_t\n",
    "        prediction_counts += 1\n",
    "\n",
    "# Calculate and print the prediction accuracy metrics\n",
    "print('Prediction accuracy MAE:', total_mae / prediction_counts)\n",
    "print('Prediction accuracy MAPE:', total_mape / prediction_counts)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction accuracy MAE: 23.868811675730655\n",
      "Prediction accuracy MAPE: 0.2442996703630106\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "#Hola"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ]
}
